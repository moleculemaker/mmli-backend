# General ingress for all endpoints (except /chemscraper/analyze)
ingress:
  hostname: mmli.fastapi.localhost
  tls: false
  annotations: {}

# Custom ingress configuration for only /chemscraper/analyze endpoint
analyzeIngress:
  hostname: mmli.fastapi.localhost
  tls: false
  annotations: {}


controller:
  image: moleculemaker/mmli-backend:latest

sharedStorage:
  existingClaim: mmli-clean-job-weights
  mountPath: /uws
  subPath: uws

# https://artifacthub.io/packages/helm/bitnami/postgresql?modal=values
postgresql:
  enabled: true
  autoschema: false
  hostname: ""
  auth:
    # Use an existing secret - this overrides username, password, and rootPassword
    # existingSecret: ""

    # Local development only - hardcode credentials here for simplicity
    # rootPassword: ""
    # password: ""

    # Set the name of your postgresql user
    username: "postgres"

    # Set the name of your postgresql database
    database: "mmli"

# https://artifacthub.io/packages/helm/bitnami/minio?modal=values
minio:
  enabled: true
  apiIngress:
    enabled: true
    tls: false
    hostname: minioapi.mmli.fastapi.localhost
    annotations: {}
  ingress:
    enabled: true
    tls: false
    hostname: minio.mmli.fastapi.localhost 
    annotations: {}
  auth:
    # Use an existing secret - this overrides username, password, and rootPassword
    # existingSecret: ""

    # Local development only - hardcode credentials here for simplicity
    # rootUser: "admin"
    # rootPassword: ""


config:
  # Change frontend service URLs depending on where they're deployed (used for email notifications)
  # TODO: Default these to local cluster? For now, default is staging and we override for prod
  chemscraper_url: "http://chemscraper-services-staging.staging.svc.cluster.local:8000"
  chemscraper_frontend_url: "https://chemscraper.frontend.staging.mmli1.ncsa.illinois.edu"
  novostoic_frontend_url: "https://novostoic.frontend.staging.mmli1.ncsa.illinois.edu"
  somn_frontend_url: "https://somn.frontend.staging.mmli1.ncsa.illinois.edu"
  clean_frontend_url: "https://clean.frontend.staging.mmli1.ncsa.illinois.edu"
  molli_frontend_url: "https://molli.frontend.staging.mmli1.ncsa.illinois.edu"

  server:
    port: 8080
    loglevel: "DEBUG"

    protocol: "https"
    ## API basepath. Must match an ingress in the ingress.basepaths list.
    apiBasePath: "api/v1"
    basePath: ""
    ## API hostname. Must match the ingress.hostname value.
    hostName: "mmli.fastapi.mmli1.ncsa.illinois.edu"

  db:
    url: ''

  minio:
    server: "mmli-backend-minio.mmli.svc.cluster.local:9000"
    apiBaseUrl: "minioapi.mmli.fastapi.localhost"
    accessKey: ''
    secretKey: ''

  auth:
    userInfoUrl: "http://oauth2.proxy.local/oauth2/userinfo"
    cookiename: "_oauth2_proxy"

  email:
    server: "smtp.ncsa.uiuc.edu"
    fromEmail: "devnull+alphasynthesis@ncsa.illinois.edu"
    fromName: "no-reply-ALPHASYNTHESIS"

  kubernetes_jobs:
    # Config for running CLEAN job
    clean:
      image: "moleculemaker/clean-image-amd64:latest"
      imagePullPolicy: "Always"
      volumes:
        # TODO: Refactor volumes in CLEAN
        - name: 'shared-storage'
          mountPath: '/app/data/inputs'
          subPath: 'uws/jobs/clean/JOB_ID/in'
          claimName: 'mmli-clean-job-weights'
        - name: 'shared-storage'
          mountPath: '/app/results/inputs'
          subPath: 'output'
          claimName: 'mmli-clean-job-weights'
        - name: 'shared-storage'
          mountPath: '/root/.cache/torch/hub/checkpoints'
          subPath: 'weights'
          claimName: 'mmli-clean-job-weights'


    # Config for running MOLLI job
    molli:
      image: "ghcr.io/moleculemaker/molli:ncsa-workflow"
      command: "/molli/entrypoint.sh"
      imagePullPolicy: "Always"
      imagePullSecrets:
        - regcred
      volumes:
        # TODO: Refactor volumes in MOLLI
        - name: 'shared-storage'
          mountPath: '/uws/jobs/clean/JOB_ID/in'
          subPath: 'uws/jobs/clean/JOB_ID/in'
          claimName: 'mmli-clean-job-weights'
        - name: 'shared-storage'
          mountPath: '/app/results/inputs'
          subPath: 'output'
          claimName: 'mmli-clean-job-weights'
        - name: 'shared-storage'
          mountPath: '/root/.cache/torch/hub/checkpoints'
          subPath: 'weights'
          claimName: 'mmli-clean-job-weights'

    # Config for running SOMN job
    somn:
      image: "ianrinehart/somn:1.0"
      projectDirectory: '/tmp/somn_root/somn_scratch/IID-Models-2024'
      command: "cp ${JOB_INPUT_DIR}/example_request.csv ${SOMN_PROJECT_DIR}/scratch/example_request.csv && micromamba run -n base somn predict last latest asdf && cp -r ${SOMN_PROJECT_DIR}/outputs/asdf/*/* ${JOB_OUTPUT_DIR}"
      imagePullPolicy: "Always"
      securityContext:
        # TODO: mount/run as mambauser => 57439 causes permission errors
        # workaround for now is to run as root
        runAsUser: 0
        runAsGroup: 0
        #fsGroup: 57439
      volumes:
        - name: 'shared-storage'
          mountPath: '/uws/jobs/somn/JOB_ID/in'
          subPath: 'uws/jobs/somn/JOB_ID/in'
          claimName: 'mmli-clean-job-weights'
        - name: 'shared-storage'
          mountPath: '/uws/jobs/somn/JOB_ID/out'
          subPath: 'uws/jobs/somn/JOB_ID/out'
          claimName: 'mmli-clean-job-weights'

    # Config for running NovoStoic job (subjob: OptStoic)
    novostoic-optstoic:
      image: "moleculemaker/novostoic"
      command: "python ./novostoic-job.py optstoic"
      imagePullPolicy: "Always"
      #imagePullSecrets:
      #  - regcred
      volumes:
        - name: 'shared-storage'
          mountPath: '/uws/jobs/novostoic-optstoic/JOB_ID/out'
          subPath: 'uws/jobs/novostoic-optstoic/JOB_ID/out'
          claimName: 'mmli-clean-job-weights'
      resources:
        limits:
          cpu: "1"
          memory: "24Gi"
        requests:
          cpu: "1"
          memory: "12Gi"

    # Config for running NovoStoic job (subjob: NovoStoic Pathways Search)
    novostoic-pathways:
      image: "moleculemaker/novostoic"
      command: "python ./novostoic-job.py pathways"
      imagePullPolicy: "Always"
      #imagePullSecrets:
      #  - regcred
      volumes:
        - name: 'shared-storage'
          mountPath: 'uws/jobs/novostoic-pathways/JOB_ID/out'
          subPath: 'uws/jobs/novostoic-pathways/JOB_ID/out'
          claimName: 'mmli-clean-job-weights'

    # Config for running NovoStoic job (subjob: dGPredictor)
    novostoic-dgpredictor:
      image: "moleculemaker/novostoic"
      command: "python ./novostoic-job.py dgpredictor"
      imagePullPolicy: "Always"
      #imagePullSecrets:
      #  - regcred
      volumes:
        - name: 'shared-storage'
          mountPath: '/uws/jobs/novostoic-dgpredictor/JOB_ID/out'
          subPath: 'uws/jobs/novostoic-dgpredictor/JOB_ID/out'
          claimName: 'mmli-clean-job-weights'

    # Config for running NovoStoic job (subjob: EnzRank)
    novostoic-enzrank:
      image: "moleculemaker/novostoic"
      command: "python ./novostoic-job.py enzrank"
      imagePullPolicy: "Always"
      #imagePullSecrets:
      #  - regcred
      volumes:
        - name: 'shared-storage'
          mountPath: '/uws/jobs/novostoic-enzrank/JOB_ID/out'
          subPath: 'uws/jobs/novostoic-enzrank/JOB_ID/out'
          claimName: 'mmli-clean-job-weights'

    defaults:
      # Kubeconfig (credentials + cluster) used to run this job in Kubernetes
      kubeconfig: "/opt/kubeconfig"
      # Namespace where this job should run
      namespace: "mmli"

      # Example job image name + command combo
      image: 'perl:5.34.0'
      command: "perl -Mbignum=bpi -wle 'print bpi(2000)' > ${JOB_OUTPUT_DIR}/pi.out"

      ## Server working directory to store generated job data
      workingVolume:
        claimName: "mmli-clean-job-weights"
        mountPath: "/uws"
        subPath: "uws"
      ## Central data volumes to be mounted in job containers
      volumes: []

      ## Abort jobs after 3 days regardless of their behavior
      activeDeadlineSeconds: 259200
      ## Cleanup completed/failed jobs after 12 hours
      ttlSecondsAfterFinished: 43200
      ## Poll the log file for changes every `logFilePollingPeriod` seconds
      logFilePollingPeriod: 300

      # TODO: probably could adjust resource limits here
      resources:
        limits:
          cpu: "1"
          memory: "16Gi"
        requests:
          cpu: "1"
          memory: "12Gi"

  external:
    chemscraper:
      apiBaseUrl: "https://chemscraper.backend.staging.mmli1.ncsa.illinois.edu"
      frontendBaseUrl: "https://chemscraper.frontend.staging.mmli1.ncsa.illinois.edu"


